{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from original Sumi and Yasseri et al. (2011) paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Part I.: making the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the dataset and things like that\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_local_xml(fp):\n",
    "    '''\n",
    "    Reads in an XML file and returns content as a list and a soup object\n",
    "\t:param fp: input filepath\n",
    "\t:return: tuple containing a list of lines and a soup object\n",
    "    '''\n",
    "    content = []\n",
    "    # Read the XML file\n",
    "    with open(fp, encoding='utf8') as file:\n",
    "        # Read each line in the file, readlines() returns a list of lines\n",
    "        content = file.readlines()\n",
    "        # Combine the lines in the list into a string\n",
    "        content_string = \"\".join(content)\n",
    "        soup = BeautifulSoup(content_string, 'xml')\n",
    "    return content, soup\n",
    "\n",
    "def lightdump_one_article(fp, article_name):\n",
    "\t'''\n",
    "\tReads in lightdump data and returns a list of all the lines of a single article\n",
    "\t:param fp: input filepath\n",
    "\t:param article_name: name of the article to return\n",
    "\t:return: list of the lines of the corresponding article\n",
    "\t'''\n",
    "\twith open(fp) as fh:\n",
    "\t    article = []\n",
    "\t    found = False\n",
    "\t    for line in fh:\n",
    "\t        if found and (line[0] != '^'):\n",
    "\t            break\n",
    "\t        if found:\n",
    "\t            article.append(line.strip())\n",
    "\t        if line.strip() == article_name:\n",
    "\t            found = True\n",
    "\tif found == False:\n",
    "\t\treturn 'Article not found'\n",
    "\n",
    "\treturn article[::-1]\n",
    "\n",
    "def xml_to_dfs(fp):\n",
    "    '''\n",
    "    Reads in an XML file and writes the data into DataFrames\n",
    "    :param fp: input filepath\n",
    "    :return: list of article titles, list of corresponding article lightdump data as DataFrame\n",
    "    '''\n",
    "    with open(fp, encoding='utf8') as file:\n",
    "        contents = file.read()\n",
    "        soup = BeautifulSoup(contents,'xml')\n",
    "        \n",
    "    titles = []\n",
    "    dfs = []\n",
    "    \n",
    "    for x in soup.findAll('page'):\n",
    "        page_text = x.findAll('text', string = True)\n",
    "        text_hash = list(map(hash, page_text))\n",
    "        count = 1\n",
    "\n",
    "        contributor_list = x.findAll('contributor')\n",
    "\n",
    "        timestamps = x.findAll('timestamp')\n",
    "\n",
    "        df = pd.DataFrame(columns = ['timestamp', 'revert', 'editNumber', 'contributor', 'hash'])\n",
    "\n",
    "        for i in range(len(text_hash)):\n",
    "            rowInfo = pd.Series(index = ['timestamp', 'revert', 'editNumber', 'contributor', 'hash'],\n",
    "                               dtype = 'object')\n",
    "\n",
    "            rowInfo['timestamp'] = timestamps[i].text\n",
    "            rowInfo['hash'] = text_hash[i]\n",
    "\n",
    "            if text_hash[i] in text_hash[:i]:\n",
    "                rowInfo['revert'] = 1\n",
    "                rowInfo['editNumber'] = df[df['hash'] == text_hash[i]]['editNumber'].iloc[0]\n",
    "\n",
    "            else:\n",
    "                rowInfo['revert'] = 0\n",
    "                rowInfo['editNumber'] = count\n",
    "                count += 1\n",
    "\n",
    "            try:\n",
    "                rowInfo['contributor'] = contributor_list[i].find('username').text\n",
    "            except:\n",
    "                rowInfo['contributor'] = contributor_list[i].find('ip').text\n",
    "\n",
    "            df = df.append(rowInfo, ignore_index = True)\n",
    "\n",
    "        title = x.find('title').text.replace(' ', '_')\n",
    "        \n",
    "        titles.append(title)\n",
    "        \n",
    "        df = df.drop('hash', axis = 1)\n",
    "        dfs.append(df)\n",
    "        \n",
    "    return titles, dfs\n",
    "\n",
    "def create_line(row):\n",
    "    '''\n",
    "    Helper function to convert DataFrame into string values for lightdump conversion\n",
    "    '''\n",
    "    line = \"^^^_\"\n",
    "\n",
    "    line = line + str(row['timestamp']) + \" \"\n",
    "    line = line + str(row['revert']) + \" \"\n",
    "    line = line + str(row['editNumber']) + \" \"\n",
    "    line = line + str(row['contributor'])\n",
    "\n",
    "    return line\n",
    "\n",
    "def write_lightdump(titles, dfs, fp):\n",
    "    '''\n",
    "    Reads in a list of titles and corresponding DataFrames\n",
    "    and writes the data into lightdump txt format\n",
    "    :param titles: list of titles\n",
    "    :param dfs: list of corresponding DataFrames\n",
    "    :param fp: output txt file path\n",
    "    '''\n",
    "    for i in range(len(titles)):\n",
    "\n",
    "        with open(fp, 'a') as file:\n",
    "            starting_line = titles[i] + '\\n'\n",
    "            \n",
    "            file.write(starting_line)\n",
    "        \n",
    "        with open(fp, 'a') as file:\n",
    "            df = dfs[i].iloc[::-1].apply(create_line, axis = 1)\n",
    "\n",
    "            df.to_csv(fp, mode = 'a', header = False, index = False)  \n",
    "            \n",
    "            file.write('\\n')\n",
    "        \n",
    "#         print(\"Writing page completed.\")\n",
    "\n",
    "            \n",
    "def lightdump_read_n(fp, n = 100):\n",
    "    '''\n",
    "\tReads in n lightdump pages and returns a list of all titles \n",
    "    read and their corresponding data as a DataFrame\n",
    "\t:param fp: input filepath\n",
    "\t:param n: number of articles to read\n",
    "\t:return: list of article titles, list of corresponding article lightdump data as DataFrame\n",
    "\t'''\n",
    "    titles = []\n",
    "    dataframes = []\n",
    "\n",
    "    with open(fp) as file:\n",
    "        df = pd.DataFrame(columns = ['timestamp', 'revert', 'revision_id', 'user'])\n",
    "        page = 0\n",
    "        for line in file:\n",
    "            if '^^^_' not in line:\n",
    "                title = line.strip('\\n').strip()\n",
    "                titles.append(title)\n",
    "\n",
    "                if title != titles[page]:\n",
    "                    page += 1\n",
    "                    \n",
    "                    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                    \n",
    "                    dataframes.append(df)\n",
    "                    \n",
    "                    df = pd.DataFrame(columns = ['timestamp', 'revert', 'revision_id', 'user'])\n",
    "\n",
    "                    if page == n:\n",
    "                        break\n",
    "            else:\n",
    "                data = line.strip(\"^^^_\").strip('\\n').split()\n",
    "                row = pd.Series(dtype = 'object')\n",
    "\n",
    "                row['timestamp'] = data[0]\n",
    "                row['revert'] = int(data[1])\n",
    "                row['revision_id'] = int(data[2])\n",
    "                row['user'] = data[3]\n",
    "\n",
    "                df = df.append(row, ignore_index = True)\n",
    "    \n",
    "    return titles, dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to turn raw data into features for modeling\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "def mstat(article):\n",
    "    '''\n",
    "    Calculates the M-statistic for a list of edits from an article\n",
    "    :param article: list of edits from an article\n",
    "    :return: M-statistic for the article\n",
    "    '''\n",
    "    revert = 0\n",
    "    revert_pairs = []\n",
    "\n",
    "    #the list of mutual reverting pairs\n",
    "    mutual_revert_pairs = []\n",
    "    #the list of unique users among reverting pairs\n",
    "    mutual_revert_users = []\n",
    "\n",
    "    #a dictionary of user as key, and his number of edits as the value\n",
    "    user_edits = {}\n",
    "\n",
    "    #a dictionary with the line number (actual version number) as key, and the line label as value(i.e. line label is either the same as version number if not revert version, or equal to an older version number if it's a revert version)\n",
    "    lineLabels = []\n",
    "    #a dictionary with the line number (actual version number) as key, and the author of that line as value\n",
    "    lineAuthors = []\n",
    "\n",
    "    ### Helper Function ###\n",
    "    def getLine(label, lineLabels):\n",
    "        for line, ll in reversed(list(enumerate(lineLabels))):\n",
    "            if lineLabels[line] == label:\n",
    "                return line\n",
    "    \n",
    "    ### Read File ###\n",
    "    for ln in article:\n",
    "        parts = ln\n",
    "        if len(parts) < 5:\n",
    "            continue\n",
    "\n",
    "        if parts[4] not in user_edits:\n",
    "            user_edits[parts[4]] = 1\n",
    "        else:\n",
    "            user_edits[parts[4]] = user_edits[parts[4]] + 1\n",
    "        if parts[2] == '1':\n",
    "            revert += 1\n",
    "            #the found line is the version i-1 equal to this version j, and the revert is assumed to be between the author of i, and j\n",
    "            line = getLine(int(parts[3]), lineLabels)\n",
    "            #ignore cases when i-1, and i are equal (consecutive versions)\n",
    "            if line >= len(lineLabels)-1:\n",
    "                continue\n",
    "            revertedU = lineAuthors[line + 1]\n",
    "            revertingU = parts[4]\n",
    "            if revertedU == revertingU:\n",
    "                continue\n",
    "            pair = revertedU + \"~!~\" + revertingU\n",
    "            if pair not in revert_pairs:\n",
    "                revert_pairs.append(pair)\n",
    "        lineLabels.append(int(parts[3]))\n",
    "        lineAuthors.append(parts[4])\n",
    "\n",
    "    ### Get Mutual ###\n",
    "    for pair in revert_pairs:\n",
    "        parts = pair.split(\"~!~\")\n",
    "        if parts[1] + \"~!~\" + parts[0] in revert_pairs:\n",
    "            sorted_pair = \"\"\n",
    "            if parts[0] < parts[1]:\n",
    "                sorted_pair = parts[0] + \"~!~\" + parts[1]\n",
    "            else:\n",
    "                sorted_pair = parts[1] + \"~!~\" + parts[0]\n",
    "                mutual_revert_pairs.append(sorted_pair)\n",
    "            if parts[1] not in mutual_revert_users:\n",
    "                mutual_revert_users.append(parts[1])\n",
    "            if parts[0] not in mutual_revert_users:\n",
    "                mutual_revert_users.append(parts[0])\n",
    "        \n",
    "        \n",
    "    #calculating the score\n",
    "    score = 0\n",
    "    pairs = []\n",
    "    for pair in list(set(mutual_revert_pairs)):\n",
    "        parts = pair.split(\"~!~\")\n",
    "        u1 = parts[0]\n",
    "        u2 = parts[1]\n",
    "        if user_edits[u1]<user_edits[u2]:\n",
    "            edit_min = user_edits[u1]\n",
    "        else:\n",
    "            edit_min = user_edits[u2]\n",
    "        pairs.append(pair + \":\" + str(edit_min))\n",
    "        score += edit_min\n",
    "\n",
    "    score *= len(mutual_revert_users)    \n",
    "    \n",
    "    return score\n",
    "\n",
    "def ld_to_sql(ld_fp, db_fp, chunksize=5000000):\n",
    "    '''\n",
    "    Converts light dump from text file to tables in a SQLite database\n",
    "    :param ld_fp: input light dump filepath\n",
    "    :param db_fp: output database filepat\n",
    "    :param chunksize: chunksize to hold in memory at a time before appending to SQL, default 50000000\n",
    "    '''\n",
    "    con = sqlite3.connect(db_fp)\n",
    "    articles_cols = ['article_id', 'article_name', 'num_edits', 'm']\n",
    "    edits_cols = ['article_id', 'timestamp', 'revert', 'edit_id', 'username']\n",
    "    pd.DataFrame(columns=articles_cols).to_sql('articles', con, if_exists='replace', index=False)\n",
    "    pd.DataFrame(columns=edits_cols).to_sql('edits', con, if_exists='replace', index=False)\n",
    "    \n",
    "    with open(ld_fp) as fh:\n",
    "        articles_data = []  \n",
    "        edits_data = []\n",
    "        article_id = 0\n",
    "        first = True                   # first article\n",
    "        num_lines = 0                  # number of lines read\n",
    "        num_edits = 0                  # number of edits in current article\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            \n",
    "            # append to sql\n",
    "            if num_lines % chunksize == 0:\n",
    "                articles_df = pd.DataFrame(articles_data, columns=articles_cols)\n",
    "                edits_df = pd.DataFrame(edits_data, columns=edits_cols)\n",
    "                articles_df.to_sql('articles', con, if_exists='append', index=False)\n",
    "                edits_df.to_sql('edits', con, if_exists='append', index=False)\n",
    "                articles_data = []                               # reset variables\n",
    "                edits_data = []\n",
    "                \n",
    "            # article name line    \n",
    "            if line[0] != '^':\n",
    "                if first:              # check if first article\n",
    "                    first = False\n",
    "                else:                  # append to lists\n",
    "                    current_article = current_article[::-1]\n",
    "                    # calculate m\n",
    "                    if num_edits < 3:\n",
    "                        m = 0\n",
    "                    else:\n",
    "                        m = mstat(current_article)\n",
    "                    articles_data.append([article_id, article_name, num_edits, m])\n",
    "                    edits_data += current_article\n",
    "                    article_id += 1\n",
    "                # reset variables\n",
    "                article_name = line\n",
    "                current_article = []\n",
    "                num_edits = 0\n",
    "            \n",
    "            # add to current article\n",
    "            else:\n",
    "                line = line.split(' ')\n",
    "                current_article.append([article_id] + line)\n",
    "                num_edits += 1\n",
    "            num_lines += 1\n",
    "        # final article\n",
    "        current_article = current_article[::-1]\n",
    "        # calculate m\n",
    "        if num_edits < 3:\n",
    "            m = 0\n",
    "        else:\n",
    "            m = mstat(current_article)\n",
    "        articles_data.append([article_id, article_name, num_edits, m])\n",
    "        edits_data += current_article\n",
    "        \n",
    "        # to sql\n",
    "        articles_df = pd.DataFrame(articles_data, columns=articles_cols)\n",
    "        edits_df = pd.DataFrame(edits_data, columns = edits_cols)\n",
    "        articles_df.to_sql('articles', con, if_exists='append', index=False)\n",
    "        edits_df.to_sql('edits', con, if_exists='append', index=False)\n",
    "        \n",
    "def query_articles(db_fp, N=None):\n",
    "    '''\n",
    "    Queries articles from the SQL database\n",
    "    :param db_fp: input database filepath\n",
    "    :param N: Number of articles to query, default all articles\n",
    "    :return: dataframe of articles read in\n",
    "    '''\n",
    "    conn = sqlite3.connect(db_fp)\n",
    "    if N:\n",
    "        df =  pd.read_sql('select * from articles limit {0}'.format(N), conn)\n",
    "    else:\n",
    "        df =  pd.read_sql('select * from articles', conn)\n",
    "    df['m'] = df['m'].astype(int)\n",
    "    df['num_edits'] = df['num_edits'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. : Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating explanatory and results oriented visualizations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def counts_vs_m_distribution_plots(articles_df, outdir):\n",
    "    '''\n",
    "    Creates violin plots cross-checking top 20 edit counts versus high m\n",
    "    :param articles_df: dataframe of articles from sql database\n",
    "    :param outdir: output directory for plots\n",
    "    :param N: number of edits and Ms to study\n",
    "    '''\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    #top 20 edited articles\n",
    "    top_20_edited = articles_df.sort_values('num_edits', ascending = False).head(20)\n",
    "\n",
    "    #top 20 m-scores\n",
    "    top_20_m = articles_df.sort_values('m', ascending = False).head(20)\n",
    "    \n",
    "    top_20_m['top 20 edited'] = top_20_m['article_name'].isin(top_20_edited['article_name'].tolist())\n",
    "    sns.violinplot(data = top_20_m, x = 'top 20 edited', y = 'm', ax=axes[0])\n",
    "    sns.violinplot(data = top_20_m, x = 'top 20 edited', y = 'num_edits', ax=axes[1])\n",
    "    \n",
    "    fig.savefig(os.path.join(outdir, 'counts_vs_m_violin.png'))\n",
    "    \n",
    "def nonzero_distribution_plots(articles_df, outdir):\n",
    "    '''\n",
    "    Creates a histogram and violin plots of the distributions of log(m) for articles with nonzero M\n",
    "    :param articles_df: dataframe of articles from sql database\n",
    "    :param outdir: output directory for plots\n",
    "    '''\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # nonzero M\n",
    "    nonzero = articles_df.copy().loc[articles_df['m'] > 0]\n",
    "    nonzero['log_m'] = np.log(nonzero['m'])\n",
    "    \n",
    "    # plot histogram\n",
    "    sns.distplot(nonzero['log_m'], ax=axes[0])\n",
    "    \n",
    "    # plot violin\n",
    "    sns.violinplot(nonzero['log_m'], ax=axes[1])\n",
    "    \n",
    "    fig.savefig(os.path.join(outdir, 'nonzero_distribution.png'))\n",
    "    \n",
    "def m_div_counts_distribution_plots(articles_df, outdir):\n",
    "    '''\n",
    "    Creates a histogram and violin plots of the distributions of (log(m) / edit counts) for articles with nonzero M\n",
    "    :param articles_df: dataframe of articles from sql database\n",
    "    :param outdir: output directory for plots\n",
    "    '''\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # nonzero M and edit counts\n",
    "    articles_df = articles_df.copy()\n",
    "    articles_df['log_m'] = np.log(articles_df['m'])\n",
    "    non_x2 = articles_df.copy().loc[(articles_df['m'] > 0) & (articles_df['num_edits'] > 0)]\n",
    "    non_x2['log_m/edits'] = non_x2['log_m']/non_x2['num_edits']\n",
    "    \n",
    "    # plot histogram\n",
    "    sns.distplot(non_x2['log_m/edits'], ax=axes[0])\n",
    "    \n",
    "    # plot violin\n",
    "    sns.violinplot(non_x2['log_m/edits'], ax=axes[1])\n",
    "    \n",
    "    fig.savefig(os.path.join(outdir, 'log_m_div_counts_distribution.png'))\n",
    "    \n",
    "def counts_vs_m_scatter_plot(articles_df, outdir):\n",
    "    '''\n",
    "    Creates a scatterplot comparing edit counts to M-stat for articles with nonzero M\n",
    "    :param articles_df: dataframe of articles from sql database\n",
    "    :param outdir: output directory for plot\n",
    "    '''\n",
    "    nonzero = articles_df.copy().loc[articles_df['m'] > 0]\n",
    "    fig = sns.regplot(data = nonzero, x = 'num_edits', y = 'm').get_figure()\n",
    "    fig.savefig(os.path.join(outdir, 'counts_vs_m_scatter.png'))\n",
    "    \n",
    "def descriptive_stats(articles_df, outdir):\n",
    "    '''\n",
    "    Descriptive statistic tables for top 20 and top 100 highest M-stat articles\n",
    "    :param articles_df: dataframe of articles from sql database\n",
    "    :param outdir: output directory for plot\n",
    "    '''\n",
    "    sorted_df = articles_df.sort_values('m', ascending = False)\n",
    "    sorted_df.head(20).describe().to_csv(os.path.join(outdir, 'top_20_stats.csv'))\n",
    "    sorted_df.head(100).describe().to_csv(os.path.join(outdir, 'top_100_stats.csv'))\n",
    "\n",
    "    \n",
    "def generate_stats(articles_df, outdir):\n",
    "    '''\n",
    "    Generates all EDA plots\n",
    "    :param articles_df: dataframe of articles from sql database\n",
    "    :param outdir: output directory for plot\n",
    "    '''\n",
    "    counts_vs_m_distribution_plots(articles_df, outdir)\n",
    "    nonzero_distribution_plots(articles_df, outdir)\n",
    "    m_div_counts_distribution_plots(articles_df, outdir)\n",
    "    counts_vs_m_scatter_plot(articles_df, outdir)\n",
    "    descriptive_stats(articles_df, outdir)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Combining all formerly defined functions [NOT READY TO RUN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.insert(0, 'src/data')\n",
    "sys.path.insert(0, 'src/features')\n",
    "sys.path.insert(0, 'src/visualization')\n",
    "\n",
    "from make_dataset import *\n",
    "from build_features import *\n",
    "from visualize import *\n",
    "\n",
    "def main(targets):\n",
    "    sql_config = json.load(open('config/data-db-params.json'))\n",
    "    eda_config = json.load(open('config/eda-params.json'))\n",
    "    all_config = json.load(open('config/all-params.json'))\n",
    "    test_config = json.load(open('config/test-params.json'))\n",
    "        \n",
    "    if 'data-db' in targets:\n",
    "        wiki_fp = sql_config['wiki_fp']\n",
    "        db_fp = sql_config['db_outfp']\n",
    "        ld_to_sql(wiki_fp, db_fp)\n",
    "        \n",
    "    if 'eda' in targets:\n",
    "        outdir = eda_config['outdir']\n",
    "        db_fp = eda_config['db_infp']\n",
    "        articles_df = query_articles(db_fp)\n",
    "        generate_stats(articles_df, outdir)\n",
    "        \n",
    "    if 'all' in targets:\n",
    "        # assumes english wikipedia light dump was downloaded into data/raw as 'en-wiki.txt'\n",
    "        lightdump_fp = all_config['data_fp']\n",
    "        db_outfp = all_config['db_outfp']\n",
    "        outdir = all_config['outdir']\n",
    "        db_infp = all_config['db_fp']\n",
    "        \n",
    "        ld_to_sql(lightdump_fp, db_outfp)\n",
    "        print('Created database from lightdump')\n",
    "        \n",
    "        articles_df = query_articles(db_infp)\n",
    "        generate_stats(articles_df, outdir)\n",
    "        print('Generated EDA plots on database')\n",
    "        \n",
    "    if 'test' in targets:\n",
    "        lightdump_fp = test_config['data_fp']\n",
    "        db_outfp = test_config['db_outfp']\n",
    "        outdir = test_config['outdir']\n",
    "        db_infp = test_config['db_fp']\n",
    "        \n",
    "        ld_to_sql(lightdump_fp, db_outfp)\n",
    "        print('Created database from lightdump')\n",
    "        \n",
    "        articles_df = query_articles(db_infp)\n",
    "        generate_stats(articles_df, outdir)\n",
    "        print('Generated EDA plots on database')\n",
    "        \n",
    "    else:\n",
    "        print('You did not pass in any arguments!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run via:\n",
    "    # python main.py data model\n",
    "    targets = sys.argv[1:]\n",
    "    main(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V.: Overview statistics of resulting data [Not run yet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import sys\n",
    "import json\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#add paths to access custom library features\n",
    "sys.path.insert(0, '../src/features')\n",
    "\n",
    "#import custom library features\n",
    "from build_features import *\n",
    "\n",
    "#read the config file\n",
    "with open('../config/eda-params.json') as f:\n",
    "    eda_config = json.load(f)\n",
    "    \n",
    "outdir = eda_config['outdir']\n",
    "db_fp = eda_config['db_infp']\n",
    "\n",
    "#read in database as DataFrame\n",
    "articles_df = query_articles(db_fp)\n",
    "\n",
    "#change type to int so we can aggregate\n",
    "articles_df['num_edits'] = articles_df['num_edits'].astype(int)\n",
    "articles_df['m'] = articles_df['m'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", str(len(articles_df)), \"on English Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptive stats for num edits\n",
    "articles_df['num_edits'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of distribution of edits for 25% of data\n",
    "articles_df.loc[articles_df['num_edits'] > 31]['num_edits'].hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['m'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.loc[articles_df['m'] > 0]['m'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages = len(articles_df)\n",
    "m_zero = articles_df.loc[articles_df['m'] == 0]\n",
    "non_zero_m_edits = len(articles_df.loc[(articles_df['m'] > 0) &\\\n",
    "                          (articles_df['num_edits'] > 0)])\n",
    "\n",
    "print(\"Total English Wikipedia pages:\", total_pages)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Pages with M-stat of 0:\", len(m_zero))\n",
    "print(\"Pages with M-stat of 0 and edit count > 0:\",\n",
    "     len(m_zero.loc[m_zero['num_edits'] > 0]))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Pages with M-stat and edit count > 0:\", non_zero_m_edits)\n",
    "print(\"Proportion of pages with M-stat and edit count > 0:\",\n",
    "      non_zero_m_edits/total_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_m = np.log(articles_df[articles_df['m'] > 0]['m'])\n",
    "\n",
    "hs = sns.distplot(log_m)\n",
    "hs.set_title('Histogram of Number of Edits/M-statistic Ratio Distribution',\n",
    "             fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = sns.violinplot(log_m)\n",
    "vp.set_title('Distribution of (log) M-statistic', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero = articles_df.loc[articles_df['m'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sns.regplot(data = nonzero, x = 'num_edits', y = 'm')\n",
    "sc.set_title('Number of Edits v. M-statistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 M-statistic Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_edited = articles_df.sort_values('num_edits', ascending = False).head(100)\n",
    "\n",
    "top_100_m = articles_df.sort_values('m', ascending = False).head(100)\n",
    "\n",
    "top_100_zero = articles_df.sort_values(['m', 'num_edits'], ascending = [True, False]).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_m.loc[top_100_m['article_name'].isin(top_100_edited['article_name'].tolist())].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_m.loc[~top_100_m['article_name'].isin(top_100_edited['article_name'].tolist())].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_edited.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = sns.distplot(log_m/nonzero['num_edits'], axlabel = \"number of edits/M-stat\")\n",
    "hs.set_title('Histogram of Number of Edits/M-statistic Ratio Distribution',\n",
    "             fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = sns.violinplot(log_m/nonzero['num_edits'])\n",
    "vp.set_title('Distribution of Number of Edits/M-statistic Ratio',\n",
    "             fontsize=12);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
